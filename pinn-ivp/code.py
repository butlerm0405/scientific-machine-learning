# -*- coding: utf-8 -*-
"""NNODE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CcbrHQ9sglNoLHwY8HyahGJuKwF6WBSr
"""

# This is a numerical solver of the ordinary differnetial equation y'(x) - 2xy(x) = 0 with initial condition y(0) = 1
# y(x) is approximated with a neural network which is trained to satisfy the differential equation and the condition. 
# The analytical solution for this ODE is y = exp(x^2), which is used in plotting to compare with the numerical solution. 

import sys
import jax.numpy as np
from jax import random
from jax import grad
from jax import vmap
from jax import jit
import matplotlib.pyplot as plt

###############
## Functions ##
###############

def softplus(x):
    return np.log(1 + np.exp(x))

def sigmoid(x):
    return 1./(1. + np.exp(-x))

def f(params, x):
    w0 = params[:80]
    b0 = params[80:160]
    w1 = params[160:240]
    b1 = params[240]
    #h = softplus(x*w0 + b0)
    h = sigmoid(x*w0 + b0)
    o = np.sum(h*w1) + b1
    return o


#1oIVP - y' - 2xy = 0

@jit
def loss(params, inputs):
    eq = dfdx_vect(params, inputs) - 2 * inputs * f_vect(params, inputs)
    bc1 = f(params, 0) - 1
    return np.mean(eq**2) + bc1**2

# Wave Equation
"""
L =1
@jit
def loss(params, inputs):
    eq = ddfddx_vect(params, inputs) +f_vect(params, inputs)
    bc1 = f(params, 0) 
    bc2 = f(params, L)
    return np.mean(eq**2) + bc1**2
"""

#2oBVP
"""
@jit
def loss(params, inputs):
    eq = -ddfddx_vect(params, inputs) - np.sin(2 * inputs) * np.cos(f_vect(params, inputs)) + 2 * f_vect(params, inputs) + 2 + np.sin(2 * inputs)*np.cos(inputs**2 -1) - 2*(inputs**2 - 1)
    bc1 = f(params, -1) 
    bc2 = f(params, 1) 
    return np.mean(eq**2) + bc1**2 + bc2**2
"""

"""
@jit
def loss(params, inputs):
    eq = ddfddx_vect(params, inputs) - np.sin(3.1415 * inputs) + 4 * f_vect(params, inputs)
    bc1 = f(params, 0) -1 
    bc2 = f(params, 1) +2 
    return np.mean(eq**2) + bc1**2 + bc2**2
"""


##########
## Main ##
##########

# Initialize Neural Network parameters

key = random.PRNGKey(0)
params = random.normal(key, shape=(241,))

# Setting up derivatives and gradients

dfdx = grad(f, 1)
ddfddx = grad(dfdx, 1)
f_vect = vmap(f, (None, 0))
dfdx_vect = vmap(dfdx, (None, 0))
ddfddx_vect = vmap(ddfddx, (None, 0))
grad_loss = jit(grad(loss, 0))

# Defining data for training
inputs = np.linspace(-1, 1, num=11)
#inputs = np.linspace(0, 1, num=401)
#inputs = random.normal(key, shape=(401,)) #Couldn't get this to work

# Fine mesh for plotting
fine_mesh = np.linspace(-1, 1, num=401)

# Setting training parameters

epochs = 20000
learning_rate = 0.0005
momentum = 0.99
velocity = 0.

PlotLoss = np.empty((20))
PlotApproxDiff = np.empty((20))
count = 0;
no_neurons_loss_holder = list()
activation_function_losses_holder = list()
# Training Neural Network

for epoch in range(epochs):
    if epoch % 100  == 0:
        print('epoch: %3d loss: %.6f' % (epoch, loss(params, inputs)))
    
    # gradient descent
    gradient = grad_loss(params + momentum*velocity, inputs)
    velocity = momentum*velocity - learning_rate*gradient
    params += velocity
    
    
    if epoch % 1000 == 0:
      Holder1 = loss(params, inputs)
      activation_function_losses_holder.append(Holder1)
    """
      Holder2 = np.sum((f_vect(params, inputs) - np.exp(inputs**2))**2)
      PlotLoss = PlotLoss.at[count].set(Holder1)
      PlotApproxDiff = PlotLoss.at[count].set(Holder2)
      count = count + 1;
    """
    """
    if epoch % 2000 == 0:
      plt.plot(inputs, np.exp(inputs**2), label='exact')
      plt.plot(inputs, f_vect(params, inputs), label='approx')
      plt.legend()
      plt.show()
    """
    # plotting finer mesh than training mesh
    if epoch % 2000 == 0:
      no_neurons_loss_holder.append(loss(params, inputs))
      plt.plot(fine_mesh, np.exp(fine_mesh**2), label='exact')
      plt.plot(fine_mesh, f_vect(params, fine_mesh), label='approx')
      plt.legend()
      plt.show()


print(np.array(no_neurons_loss_holder))
print(np.array(activation_function_losses_holder))
print(f"Final Error: {loss(params, inputs)} " )

# final plot
#plt.plot(inputs, np.exp(inputs**2), label='exact')
#plt.plot(inputs, inputs**2 -1, label='exact')
#plt.plot(inputs, f_vect(params, inputs), label='approx')
plt.plot(fine_mesh, np.exp(fine_mesh**2), label='exact')
plt.plot(fine_mesh, f_vect(params, fine_mesh), label='approx')
plt.legend()
plt.show()

# Loss plot 

LossInputs = np.linspace(1, 20, num=20)
plt.loglog(LossInputs, PlotLoss)
plt.title('Loss vs. Epochs')
plt.xlabel('Epochs every 500')
plt.ylabel('Loss')
plt.show()

plt.semilogy(LossInputs, PlotLoss)
plt.title('Loss vs. Epochs')
plt.xlabel('Epochs every 500')
plt.ylabel('Loss')
plt.show()

plt.plot(LossInputs, PlotLoss)
plt.title('Loss vs. Epochs')
plt.xlabel('Epochs every 500')
plt.ylabel('Loss')
plt.show